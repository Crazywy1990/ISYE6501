# Bryson Cook
# HW 5
# ISYE651, Spring 2018
#Part 7.2
rm(list = ls())
cat("\014")
set.seed(1)
#install.packages("stats")
library(stats)
input = data.frame(read.table("uscrime.txt", header = TRUE)) #read in data
mydata = input[c(16, 1:15)] #reorder so that crime is the first column (for formula)
#Scale the data
scaled_data = mydata
for (i in 1:16) {
scaled_data[, i] = (mydata[, i] - min(mydata[, i])) / (max(mydata[, i]) -
min(mydata[, i]))
}
point = data.frame(
M = 14.0,
So = 0,
Ed = 10.0,
Po1 = 12.0,
Po2 = 15.5,
LF = 0.640,
M.F = 94.0,
Pop = 150,
NW = 1.1,
U1 = 0.120,
U2 = 3.6,
Wealth = 3200,
Ineq = 20.1,
Prob = 0.04,
Time = 39.0
)
scaled_point = point
for (i in 1:15) {
scaled_point[1, i] = (point[1, i] -min(mydata[, i + 1])) / (max(mydata[, i +1]) -min(mydata[, i +1]))}
f = formula(scaled_data)
sc_reg = lm(f, scaled_data)
summary(sc_reg)
crime_prediction = (predict.lm(sc_reg,scaled_point) * (max(mydata[, "Crime"]) - min(mydata[, "Crime"]))) +
min(mydata[, "Crime"])
#that answer of 155 is really low, we are probably overfit, so le's look at the p-values of each point.
sc_Pvalues = summary(sc_reg)$coefficients[,4]
sc_coef = sc_reg$coefficients
# Eliminate those predictors with a p-value > 0.06.  I know  0.05 is usually the rule,
# but the U2 factor (unemployment rate of urban males 35–39) has a P value just above .05
# and should be left in.
#This is done easiest by setting the entire column = 0, so it has no impact
scaled_data_adj = scaled_data
for (i in 2:16) {
if (sc_Pvalues[i] > 0.06) {
scaled_data_adj[,i] = 0
}
}
sc_a_0 = matrix(sc_coef_adj[1])
sc_a_n = matrix(sc_coef_adj[2:16])
sc_reg_adj = lm(f, scaled_data_adj)
summary(sc_reg_adj)
crime_prediction_adj = (predict.lm(sc_reg_adj,scaled_point) * (max(mydata[, "Crime"]) - min(mydata[, "Crime"]))) +
min(mydata[, "Crime"])
crime_prediction_adj
setwd("~/Bryson/Github/ISYE6501/Homework 5")
# Bryson Cook
# HW 5
# ISYE651, Spring 2018
#Part 7.2
rm(list = ls())
cat("\014")
set.seed(1)
#install.packages("stats")
library(stats)
input = data.frame(read.table("uscrime.txt", header = TRUE)) #read in data
mydata = input[c(16, 1:15)] #reorder so that crime is the first column (for formula)
#Scale the data
scaled_data = mydata
for (i in 1:16) {
scaled_data[, i] = (mydata[, i] - min(mydata[, i])) / (max(mydata[, i]) -
min(mydata[, i]))
}
point = data.frame(
M = 14.0,
So = 0,
Ed = 10.0,
Po1 = 12.0,
Po2 = 15.5,
LF = 0.640,
M.F = 94.0,
Pop = 150,
NW = 1.1,
U1 = 0.120,
U2 = 3.6,
Wealth = 3200,
Ineq = 20.1,
Prob = 0.04,
Time = 39.0
)
scaled_point = point
for (i in 1:15) {
scaled_point[1, i] = (point[1, i] -min(mydata[, i + 1])) / (max(mydata[, i +1]) -min(mydata[, i +1]))}
f = formula(scaled_data)
sc_reg = lm(f, scaled_data)
summary(sc_reg)
crime_prediction = (predict.lm(sc_reg,scaled_point) * (max(mydata[, "Crime"]) - min(mydata[, "Crime"]))) +
min(mydata[, "Crime"])
#that answer of 155 is really low, we are probably overfit, so le's look at the p-values of each point.
sc_Pvalues = summary(sc_reg)$coefficients[,4]
sc_coef = sc_reg$coefficients
# Eliminate those predictors with a p-value > 0.06.  I know  0.05 is usually the rule,
# but the U2 factor (unemployment rate of urban males 35–39) has a P value just above .05
# and should be left in.
#This is done easiest by setting the entire column = 0, so it has no impact
scaled_data_adj = scaled_data
for (i in 2:16) {
if (sc_Pvalues[i] > 0.06) {
scaled_data_adj[,i] = 0
}
}
sc_a_0 = matrix(sc_coef_adj[1])
sc_a_n = matrix(sc_coef_adj[2:16])
sc_reg_adj = lm(f, scaled_data_adj)
summary(sc_reg_adj)
crime_prediction_adj = (predict.lm(sc_reg_adj,scaled_point) * (max(mydata[, "Crime"]) - min(mydata[, "Crime"]))) +
min(mydata[, "Crime"])
crime_prediction_adj
View(scaled_data_adj)
# Bryson Cook
# HW 5
# ISYE651, Spring 2018
#Part 7.2
rm(list = ls())
cat("\014")
set.seed(1)
#install.packages("stats")
library(stats)
input = data.frame(read.table("uscrime.txt", header = TRUE)) #read in data
mydata = input[c(16, 1:15)] #reorder so that crime is the first column (for formula)
#Scale the data
scaled_data = mydata
for (i in 1:16) {
scaled_data[, i] = (mydata[, i] - min(mydata[, i])) / (max(mydata[, i]) -
min(mydata[, i]))
}
point = data.frame(
M = 14.0,
So = 0,
Ed = 10.0,
Po1 = 12.0,
Po2 = 15.5,
LF = 0.640,
M.F = 94.0,
Pop = 150,
NW = 1.1,
U1 = 0.120,
U2 = 3.6,
Wealth = 3200,
Ineq = 20.1,
Prob = 0.04,
Time = 39.0
)
scaled_point = point
for (i in 1:15) {
scaled_point[1, i] = (point[1, i] -min(mydata[, i + 1])) / (max(mydata[, i +1]) -min(mydata[, i +1]))}
f = formula(scaled_data)
sc_reg = lm(f, scaled_data)
summary(sc_reg)
crime_prediction = (predict.lm(sc_reg,scaled_point) * (max(mydata[, "Crime"]) - min(mydata[, "Crime"]))) +
min(mydata[, "Crime"])
#that answer of 155 is really low, we are probably overfit, so le's look at the p-values of each point.
sc_Pvalues = summary(sc_reg)$coefficients[,4]
sc_coef = sc_reg$coefficients
# Eliminate those predictors with a p-value > 0.06.  I know  0.05 is usually the rule,
# but the U2 factor (unemployment rate of urban males 35–39) has a P value just above .05
# and should be left in.
#This is done easiest by setting the entire column = 0, so it has no impact
scaled_data_adj = scaled_data
for (i in 1:16) {
if (sc_Pvalues[i] > 0.06) {
scaled_data_adj[,i] = 0
}
}
sc_a_0 = matrix(sc_coef_adj[1])
sc_a_n = matrix(sc_coef_adj[2:16])
sc_reg_adj = lm(f, scaled_data_adj)
summary(sc_reg_adj)
crime_prediction_adj = (predict.lm(sc_reg_adj,scaled_point) * (max(mydata[, "Crime"]) - min(mydata[, "Crime"]))) +
min(mydata[, "Crime"])
crime_prediction_adj
View(scaled_data)
View(scaled_data_adj)
sc_Pvalues
for (i in "M",
"So",
"Ed",
"Po1",
"Po2",
"LF",
"M.F",
"Pop",
"NW",
"U1",
"U2",
"Wealth",
"Ineq",
"Prob",
"Time") {
if (sc_Pvalues[i] > 0.06) {
scaled_data_adj[, i] = 0
}
}
# Bryson Cook
# HW 5
# ISYE651, Spring 2018
#Part 7.2
rm(list = ls())
cat("\014")
set.seed(1)
#install.packages("stats")
library(stats)
input = data.frame(read.table("uscrime.txt", header = TRUE)) #read in data
mydata = input[c(16, 1:15)] #reorder so that crime is the first column (for formula)
#Scale the data
scaled_data = mydata
for (i in 1:16) {
scaled_data[, i] = (mydata[, i] - min(mydata[, i])) / (max(mydata[, i]) -
min(mydata[, i]))
}
point = data.frame(
M = 14.0,
So = 0,
Ed = 10.0,
Po1 = 12.0,
Po2 = 15.5,
LF = 0.640,
M.F = 94.0,
Pop = 150,
NW = 1.1,
U1 = 0.120,
U2 = 3.6,
Wealth = 3200,
Ineq = 20.1,
Prob = 0.04,
Time = 39.0
)
scaled_point = point
for (i in 1:15) {
scaled_point[1, i] = (point[1, i] -min(mydata[, i + 1])) / (max(mydata[, i +1]) -min(mydata[, i +1]))}
f = formula(scaled_data)
sc_reg = lm(f, scaled_data)
summary(sc_reg)
crime_prediction = (predict.lm(sc_reg,scaled_point) * (max(mydata[, "Crime"]) - min(mydata[, "Crime"]))) +
min(mydata[, "Crime"])
#that answer of 155 is really low, we are probably overfit, so le's look at the p-values of each point.
sc_Pvalues = summary(sc_reg)$coefficients[,4]
sc_coef = sc_reg$coefficients
# Eliminate those predictors with a p-value > 0.06.  I know  0.05 is usually the rule,
# but the U2 factor (unemployment rate of urban males 35–39) has a P value just above .05
# and should be left in.
#This is done easiest by setting the entire column = 0, so it has no impact
scaled_data_adj = scaled_data
# for (i in 1:16) {
#   if (sc_Pvalues[i] > 0.06) {
#     scaled_data_adj[,i] = 0
#   }
# }
for (i in "M",
"So",
"Ed",
"Po1",
"Po2",
"LF",
"M.F",
"Pop",
"NW",
"U1",
"U2",
"Wealth",
"Ineq",
"Prob",
"Time") {
if (sc_Pvalues[i] > 0.06) {
scaled_data_adj[, i] = 0
}
}
# sc_a_0 = matrix(sc_coef_adj[1])
# sc_a_n = matrix(sc_coef_adj[2:16])
sc_reg_adj = lm(f, scaled_data_adj)
summary(sc_reg_adj)
crime_prediction_adj = (predict.lm(sc_reg_adj,scaled_point) * (max(mydata[, "Crime"]) - min(mydata[, "Crime"]))) +
min(mydata[, "Crime"])
crime_prediction_adj
sc_Pvalues
# Bryson Cook
# HW 5
# ISYE651, Spring 2018
#Part 7.2
rm(list = ls())
cat("\014")
set.seed(1)
#install.packages("stats")
library(stats)
input = data.frame(read.table("uscrime.txt", header = TRUE)) #read in data
mydata = input[c(16, 1:15)] #reorder so that crime is the first column (for formula)
#Scale the data
scaled_data = mydata
for (i in 1:16) {
scaled_data[, i] = (mydata[, i] - min(mydata[, i])) / (max(mydata[, i]) -
min(mydata[, i]))
}
point = data.frame(
M = 14.0,
So = 0,
Ed = 10.0,
Po1 = 12.0,
Po2 = 15.5,
LF = 0.640,
M.F = 94.0,
Pop = 150,
NW = 1.1,
U1 = 0.120,
U2 = 3.6,
Wealth = 3200,
Ineq = 20.1,
Prob = 0.04,
Time = 39.0
)
scaled_point = point
for (i in 1:15) {
scaled_point[1, i] = (point[1, i] -min(mydata[, i + 1])) / (max(mydata[, i +1]) -min(mydata[, i +1]))}
f = formula(scaled_data)
sc_reg = lm(f, scaled_data)
summary(sc_reg)
crime_prediction = (predict.lm(sc_reg,scaled_point) * (max(mydata[, "Crime"]) - min(mydata[, "Crime"]))) +
min(mydata[, "Crime"])
#that answer of 155 is really low, we are probably overfit, so le's look at the p-values of each point.
sc_Pvalues = summary(sc_reg)$coefficients[,4]
sc_coef = sc_reg$coefficients
# Eliminate those predictors with a p-value > 0.06.  I know  0.05 is usually the rule,
# but the U2 factor (unemployment rate of urban males 35–39) has a P value just above .05
# and should be left in.
#This is done easiest by setting the entire column = 0, so it has no impact
scaled_data_adj = scaled_data
for (i in 2:16) {
if (sc_Pvalues[i] > 0.06) {
scaled_data_adj[,i] = 0
}
}
#
# for (i in (
#   "M",
#   "So",
#   "Ed",
#   "Po1",
#   "Po2",
#   "LF",
#   "M.F",
#   "Pop",
#   "NW",
#   "U1",
#   "U2",
#   "Wealth",
#   "Ineq",
#   "Prob",
#   "Time"
# )) {
#   if (sc_Pvalues[i] > 0.06) {
#     scaled_data_adj[, i] = 0
#   }
# }
# sc_a_0 = matrix(sc_coef_adj[1])
# sc_a_n = matrix(sc_coef_adj[2:16])
sc_reg_adj = lm(f, scaled_data_adj)
summary(sc_reg_adj)
crime_prediction_adj = (predict.lm(sc_reg_adj,scaled_point) * (max(mydata[, "Crime"]) - min(mydata[, "Crime"]))) +
min(mydata[, "Crime"])
crime_prediction_adj
View(scaled_data)
View(scaled_data_adj)
