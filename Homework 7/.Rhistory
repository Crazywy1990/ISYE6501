data.frame(summary(model2)$coef[summary(model2)$coef[, 4] <= .05, 4])
f3 = V21 ~ factor(V1) + V2 + factor(V3) + factor(V4) + V8 + factor(V9) + factor(V10) + factor(V14) + factor(V15) +  factor(V20)
model3 = glm(f3, family = binomial(link = "logit"), data = train)
summary(model3)
model3$coefficients
#Apply cross testidation to double check which model is better:
cv = cv.glm(train, model, K = 10)
cv2 = cv.glm(train, model2, K = 10)
cv3 = cv.glm(train, model3, K = 10)
cv$delta[1]
cv2$delta[1]
cv3$delta[1]
# Now we check what the response threshold should be using the test data:
pred = predict.glm(model3, newdata = test, type = 'response')
actual = test$V21
totalcost = matrix(, 100, ncol = 2)
n = 1
for (thresh in seq(.01, .99, .01)) {
answerx = matrix(, nrow(test), ncol = 1)
for (x in 1:length(pred)) {
if (pred[x] >= thresh) {
answerx[x] = 1
} else
answerx[x] = 0
}
cm = confusionMatrix(data = answerx,
reference = actual,
positive = '0')
# Now minimize the total cost, which would be cost = FP*5 + FN*1
cost = cm$table[2, 1] * 1 + cm$table[1, 2] * 5
totalcost[n, 1] = thresh
totalcost[n, 2] = cost
n = n + 1
}
totalcost
totalcost[which.min(totalcost[, 2]), 1]
# From this, the minimum cost is found to be 57 at a minimum threshold of 0.86.
thresh = .86
answerx = matrix(, nrow(test), ncol = 1)
for (x in 1:length(pred)) {
if (pred[x] >= thresh) {
answerx[x] = 1
} else
answerx[x] = 0
}
cm = confusionMatrix(data = answerx,
reference = actual,
positive = '0')
cm
#from the confusion matrix we see 142 TN,
roc = roc(test$V21, answerx)
roc
plot(roc)
# Bryson Cook
# HW 7
# ISYE651, Spring 2018
#Part 10.1(b) - Logistic Regression
rm(list = ls())
cat("\014")
set.seed(1)
# install.packages("stats")
# install.packages("boot")
# install.packages("caret")
# install.packages("pROC")
library(stats)
library(boot)
library(caret)
library(pROC)
input = data.frame(read.table("germancredit.txt", header = F)) #read in data
# http://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29
loan = input[21] - 1 #scale the result vector to between 0 and 1
mydata = cbind(loan, input[1:20]) #reorder so that the result is the first column (for formula)
r = nrow(mydata)
set = sample(1:r, size = round(r * .8), replace = FALSE)
train = mydata[set, ]
test = mydata[-set, ]
#build the model, with categorical factors being treated with the factor() function.
f1 = V21 ~ factor(V1) + V2 + factor(V3) + factor(V4) + V5 + factor(V6) + factor(V7) + V8 + factor(V9) + factor(V10) + V11 + factor(V12) + V13 + factor(V14) + factor(V15) + V16 + factor(V17) + V18 + factor(V19) + factor(V20)
predictors = train[-1]
loan = train[1] - 1
model = glm(f1, family = binomial(link = "logit"), data = train)
summary(model)
# Find and eliminate those predictors with a p-testue < 0.05
data.frame(summary(model)$coef[summary(model)$coef[, 4] <= .05, 4])
f2 = V21 ~ factor(V1) + V2 + factor(V3) + factor(V4) + V5 + factor(V6) + V8 + factor(V9) + factor(V10) + factor(V12) + factor(V14) + factor(V15) + factor(V16) +  factor(V20)
model2 = glm(f2, family = binomial(link = "logit"), data = train)
summary(model2)
#Again, find and eliminate those predictors with a p-testue < 0.05
data.frame(summary(model2)$coef[summary(model2)$coef[, 4] <= .05, 4])
f3 = V21 ~ factor(V1) + V2 + factor(V3) + factor(V4) + V8 + factor(V9) + factor(V10) + factor(V14) + factor(V15) +  factor(V20)
model3 = glm(f3, family = binomial(link = "logit"), data = train)
summary(model3)
model3$coefficients
#Apply cross testidation to double check which model is better:
cv = cv.glm(train, model, K = 10)
cv2 = cv.glm(train, model2, K = 10)
cv3 = cv.glm(train, model3, K = 10)
cv$delta[1]
cv2$delta[1]
cv3$delta[1]
# Now we check what the response threshold should be using the test data:
pred = predict.glm(model3, newdata = test, type = 'response')
actual = test$V21
totalcost = matrix(, 100, ncol = 2)
n = 1
for (thresh in seq(.01, .99, .01)) {
answerx = matrix(, nrow(test), ncol = 1)
for (x in 1:length(pred)) {
if (pred[x] >= thresh) {
answerx[x] = 1
} else
answerx[x] = 0
}
cm = confusionMatrix(data = answerx,
reference = actual,
positive = '0')
# Now minimize the total cost, which would be cost = FP*5 + FN*1
cost = cm$table[2, 1] * 5 + cm$table[1, 2] * 1
totalcost[n, 1] = thresh
totalcost[n, 2] = cost
n = n + 1
}
totalcost
totalcost[which.min(totalcost[, 2]), 1]
# From this, the minimum cost is found to be 57 at a minimum threshold of 0.86.
thresh = .86
answerx = matrix(, nrow(test), ncol = 1)
for (x in 1:length(pred)) {
if (pred[x] >= thresh) {
answerx[x] = 1
} else
answerx[x] = 0
}
cm = confusionMatrix(data = answerx,
reference = actual,
positive = '0')
cm
#from the confusion matrix we see 142 TN,
roc = roc(test$V21, answerx)
roc
plot(roc)
19*5+55
roc = roc(as.matrix(test$V21), as.matrix(answerx)
roc = roc(as.matrix(test$V21), as.matrix(answerx))
test$V21
answerx
roc = roc(test$V21, answerx[,1])
roc = roc(test$V21, answerx[,1])
roc
plot(roc)
# Bryson Cook
# HW 7
# ISYE651, Spring 2018
#Part 10.1(b) - Logistic Regression
rm(list = ls())
cat("\014")
set.seed(1)
# install.packages("stats")
# install.packages("boot")
# install.packages("caret")
# install.packages("pROC")
library(stats)
library(boot)
library(caret)
library(pROC)
input = data.frame(read.table("germancredit.txt", header = F)) #read in data
# http://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29
loan = input[21] - 1 #scale the result vector to between 0 and 1
mydata = cbind(loan, input[1:20]) #reorder so that the result is the first column (for formula)
r = nrow(mydata)
set = sample(1:r, size = round(r * .8), replace = FALSE)
train = mydata[set,]
test = mydata[-set,]
#build the model, with categorical factors being treated with the factor() function.
f1 = V21 ~ factor(V1) + V2 + factor(V3) + factor(V4) + V5 + factor(V6) + factor(V7) + V8 + factor(V9) + factor(V10) + V11 + factor(V12) + V13 + factor(V14) + factor(V15) + V16 + factor(V17) + V18 + factor(V19) + factor(V20)
predictors = train[-1]
loan = train[1] - 1
model = glm(f1, family = binomial(link = "logit"), data = train)
summary(model)
# Find and eliminate those predictors with a p-testue < 0.05
data.frame(summary(model)$coef[summary(model)$coef[, 4] <= .05, 4])
f2 = V21 ~ factor(V1) + V2 + factor(V3) + factor(V4) + V5 + factor(V6) + V8 + factor(V9) + factor(V10) + factor(V12) + factor(V14) + factor(V15) + factor(V16) +  factor(V20)
model2 = glm(f2, family = binomial(link = "logit"), data = train)
summary(model2)
#Again, find and eliminate those predictors with a p-testue < 0.05
data.frame(summary(model2)$coef[summary(model2)$coef[, 4] <= .05, 4])
f3 = V21 ~ factor(V1) + V2 + factor(V3) + factor(V4) + V8 + factor(V9) + factor(V10) + factor(V14) + factor(V15) +  factor(V20)
model3 = glm(f3, family = binomial(link = "logit"), data = train)
summary(model3)
model3$coefficients
#Apply cross testidation to double check which model is better:
cv = cv.glm(train, model, K = 10)
cv2 = cv.glm(train, model2, K = 10)
cv3 = cv.glm(train, model3, K = 10)
cv$delta[1]
cv2$delta[1]
cv3$delta[1]
# Now we check what the response threshold should be using the test data:
pred = predict.glm(model3, newdata = test, type = 'response')
actual = test$V21
totalcost = matrix(, 100, ncol = 2)
n = 1
for (thresh in seq(.01, .99, .01)) {
answerx = matrix(, nrow(test), ncol = 1)
for (x in 1:length(pred)) {
if (pred[x] >= thresh) {
answerx[x] = 1
} else
answerx[x] = 0
}
cm = confusionMatrix(data = answerx,
reference = actual,
positive = '0')
# Now minimize the total cost, which would be cost = FP*5 + FN*1
cost = cm$table[2, 1] * 5 + cm$table[1, 2] * 1
totalcost[n, 1] = thresh
totalcost[n, 2] = cost
n = n + 1
}
totalcost
totalcost[which.min(totalcost[, 2]), 1]
# From this, the minimum cost is found to be 57 at a minimum threshold of 0.86.
thresh = .86
answerx = matrix(, nrow(test), ncol = 1)
for (x in 1:length(pred)) {
if (pred[x] >= thresh) {
answerx[x] = 1
} else
answerx[x] = 0
}
cm = confusionMatrix(data = answerx,
reference = actual,
positive = '0')
cm
#from the confusion matrix we see 142 TN,
roc = roc(test$V21, answerx[, 1])
roc
plot(roc)
roc = roc(test$V21, pred)
roc
plot(roc)
thresh = .4
answerx = matrix(, nrow(test), ncol = 1)
for (x in 1:length(pred)) {
if (pred[x] >= thresh) {
answerx[x] = 1
} else
answerx[x] = 0
}
cm = confusionMatrix(data = answerx,
reference = actual,
positive = '0')
cm
#from the confusion matrix we see 142 TN,
roc = roc(test$V21, pred)
roc
plot(roc)
thresh = .6
answerx = matrix(, nrow(test), ncol = 1)
for (x in 1:length(pred)) {
if (pred[x] >= thresh) {
answerx[x] = 1
} else
answerx[x] = 0
}
cm = confusionMatrix(data = answerx,
reference = actual,
positive = '0')
cm
#from the confusion matrix we see 142 TN,
roc = roc(test$V21, pred)
roc
plot(roc)
thresh = .8
answerx = matrix(, nrow(test), ncol = 1)
for (x in 1:length(pred)) {
if (pred[x] >= thresh) {
answerx[x] = 1
} else
answerx[x] = 0
}
cm = confusionMatrix(data = answerx,
reference = actual,
positive = '0')
cm
#from the confusion matrix we see 142 TN,
roc = roc(test$V21, answerx)
roc
plot(roc)
thresh = .4
answerx = matrix(, nrow(test), ncol = 1)
for (x in 1:length(pred)) {
if (pred[x] >= thresh) {
answerx[x] = 1
} else
answerx[x] = 0
}
cm = confusionMatrix(data = answerx,
reference = actual,
positive = '0')
cm
#from the confusion matrix we see 142 TN,
roc = roc(test$V21, answerx)
roc
plot(roc)
thresh = .86
answerx = matrix(, nrow(test), ncol = 1)
for (x in 1:length(pred)) {
if (pred[x] >= thresh) {
answerx[x] = 1
} else
answerx[x] = 0
}
cm = confusionMatrix(data = answerx,
reference = actual,
positive = '0')
cm
#from the confusion matrix we see 142 TN,
roc = roc(test$V21, answerx)
roc
plot(roc)
# Bryson Cook
# HW 7
# ISYE651, Spring 2018
#Part 10.1(b) - Logistic Regression
rm(list = ls())
cat("\014")
set.seed(1)
# install.packages("stats")
# install.packages("boot")
# install.packages("caret")
# install.packages("pROC")
library(stats)
library(boot)
library(caret)
library(pROC)
input = data.frame(read.table("germancredit.txt", header = F)) #read in data
# http://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29
loan = input[21] - 1 #scale the result vector to between 0 and 1
mydata = cbind(loan, input[1:20]) #reorder so that the result is the first column (for formula)
r = nrow(mydata)
set = sample(1:r, size = round(r * .8), replace = FALSE)
train = mydata[set,]
test = mydata[-set,]
#build the model, with categorical factors being treated with the factor() function.
f1 = V21 ~ factor(V1) + V2 + factor(V3) + factor(V4) + V5 + factor(V6) + factor(V7) + V8 + factor(V9) + factor(V10) + V11 + factor(V12) + V13 + factor(V14) + factor(V15) + V16 + factor(V17) + V18 + factor(V19) + factor(V20)
predictors = train[-1]
loan = train[1] - 1
model = glm(f1, family = binomial(link = "logit"), data = train)
summary(model)
# Find and eliminate those predictors with a p-testue < 0.05
data.frame(summary(model)$coef[summary(model)$coef[, 4] <= .05, 4])
f2 = V21 ~ factor(V1) + V2 + factor(V3) + factor(V4) + V5 + factor(V6) + V8 + factor(V9) + factor(V10) + factor(V12) + factor(V14) + factor(V15) + factor(V16) +  factor(V20)
model2 = glm(f2, family = binomial(link = "logit"), data = train)
summary(model2)
#Again, find and eliminate those predictors with a p-testue < 0.05
data.frame(summary(model2)$coef[summary(model2)$coef[, 4] <= .05, 4])
f3 = V21 ~ factor(V1) + V2 + factor(V3) + factor(V4) + V8 + factor(V9) + factor(V10) + factor(V14) + factor(V15) +  factor(V20)
model3 = glm(f3, family = binomial(link = "logit"), data = train)
summary(model3)
model3$coefficients
#Apply cross testidation to double check which model is better:
cv = cv.glm(train, model, K = 10)
cv2 = cv.glm(train, model2, K = 10)
cv3 = cv.glm(train, model3, K = 10)
cv$delta[1]
cv2$delta[1]
cv3$delta[1]
# Now we check what the response threshold should be using the test data:
pred = predict.glm(model3, newdata = test, type = 'response')
actual = test$V21
totalcost = matrix(, 100, ncol = 2)
n = 1
for (thresh in seq(.01, .99, .01)) {
answerx = matrix(, nrow(test), ncol = 1)
for (x in 1:length(pred)) {
if (pred[x] >= thresh) {
answerx[x] = 1
} else
answerx[x] = 0
}
cm = confusionMatrix(data = answerx,
reference = actual,
positive = '0')
# Now minimize the total cost, which would be cost = FP*5 + FN*1
cost = cm$table[2, 1] * 5 + cm$table[1, 2] * 1
totalcost[n, 1] = thresh
totalcost[n, 2] = cost
n = n + 1
}
totalcost
totalcost[which.min(totalcost[, 2]), 1]
# From this, the minimum cost is found to be 57 at a minimum threshold of 0.86.
thresh = .86
answerx = matrix(, nrow(test), ncol = 1)
for (x in 1:length(pred)) {
if (pred[x] >= thresh) {
answerx[x] = 1
} else
answerx[x] = 0
}
cm = confusionMatrix(data = answerx,
reference = actual,
positive = '0')
cm
#from the confusion matrix we see 142 TN, 57 FN, 0 FP, and 1 TP
roc = roc(test$V21, answerx)
roc
plot(roc)
# Bryson Cook
# HW 7
# ISYE651, Spring 2018
#Part 10.1(a) - Regression Tree
rm(list = ls())
cat("\014")
set.seed(1)
# install.packages("tree")
# install.packages("rpart")
library(tree)
library(rpart)
input = data.frame(read.table("uscrime.txt", header = TRUE)) #read in data
mydata = input[c(16, 1:15)] #reorder so that crime is the first column (for formula)
f1 = formula(mydata)
predictors = mydata[-1]
crime = mydata[1]
data_train = mydata[1:35,]
data_test = mydata[36:nrow(mydata),]
tree = rpart(Crime ~ ., data = data_train)
summary(tree) # detailed summary of splits
print(tree)
plot(tree, uniform = TRUE, main = "Crime Rate Data Decision Tree")
text(tree,
use.n = TRUE,
all = TRUE,
cex = .8)
tree$where
names = row.names(tree$frame)
loc = names[tree$where]
groups = cbind(loc, data_train)
pred = predict(tree, data_test)
sse = sum((pred - data_test$Crime) ^ 2)
sst = sum((data_test$Crime - mean(data_test$Crime)) ^ 2) #total sum of squares
1 - sse / sst
# Sum of squares error is greater than the total sum of squares.  This is a bad model.
# Try pruning to the lowest cross-validation error from the original tree.
ptree = prune(tree, cp = 0.1481)
plot(ptree, uniform = TRUE, main = "Crime Rate Data Decision Tree (Pruned)")
text(ptree,
use.n = TRUE,
all = TRUE,
cex = .8)
summary(ptree)
print(ptree)
pnames = row.names(ptree$frame)
ploc = pnames[ptree$where]
pgroups = cbind(ploc, data_train)
ppred = predict(ptree, data_test)
psse = sum((ppred - data_test$Crime) ^ 2) #sum of squared errors
psst = sum((data_test$Crime - mean(data_test$Crime)) ^ 2) #total sum of squares
1 - psse / psst
pgroup2 = subset.data.frame(pgroups, ploc == 2)
pmodel2 = lm(f1, pgroup2)
summary(pmodel2)
#From the summary, reduce the number of factors by pvalue:
pmodel2_a = lm(Crime ~ Wealth + Time + M + So + M.F + Prob, pgroup2)
summary(pmodel2_a) # the model is now worse. I think it is just due to extreme overfitting
pgroup3 = subset.data.frame(pgroups, ploc == 3)
pmodel3 = lm(f1, pgroup3)
summary(pmodel3) #This model is so overfit that 5 coeficients are not defined. Reducing the factors gives:
pmodel3_a = lm(Crime ~ M + So + Ed + Po1 + Po2 + LF + M.F + Pop + NW, pgroup2)
summary(pmodel3_a)
# I would think we are experiencing a large amount
# of over-fitting since the node, pgroup3, only contains 11 points of the original data, which is very
# small but still above the 5% minimum (thought 5% of 47 is only 2-3 points of data). The psuedo-R2 calculated
# from the test data still shows that this model is just not good at all.
# Bryson Cook
# HW 7
# ISYE651, Spring 2018
#Part 10.1(a) - Random Forest
rm(list = ls())
cat("\014")
set.seed(1)
#install.packages("randomForest")
library(randomForest)
input = data.frame(read.table("uscrime.txt", header = TRUE)) #read in data
mydata = input[c(16, 1:15)] #reorder so that crime is the first column (for formula)
f1 = formula(mydata)
predictors = mydata[-1]
crime = mydata[1]
fit = randomForest(f1, data = mydata, importance = TRUE)
print(fit)
plot()
varImpPlot(fit, main = "Crime Rate Random Forest Variable Importance")
importance(fit)
# Interesting that the Po1, Po2, NW, and Prob variables are considered important in this model,
# as they were not in the previous HW's.
pred = predict(fit)
sse = sum((pred - mydata$Crime) ^ 2)
sst = sum((mydata$Crime - mean(mydata$Crime)) ^ 2)
1 - sse / sst
# This seems much better than the models found in the regular regression tree. Random forests
# have the benefit of reducing overfitting.  Let's try splitting the data into training
# and testing groups
data_train = mydata[1:37, ]
data_test = mydata[38:nrow(mydata), ]
fit2 = randomForest(f1, data = data_train, importance = TRUE)
pred2 = predict(fit2, data_test)
psse = sum((pred2 - data_test$Crime) ^ 2)
psst = sum((data_test$Crime - mean(data_test$Crime)) ^ 2)
1 - psse / psst
# The R2 is much lower than the original, at 0.265.  This decrease is expected though, since
# we are not validating on the same data.
