> set.seed(1)
> 
> # install.packages("tree")
> # install.packages("rpart")
> 
> library(tree)
> library(rpart)
> 
> 
> input = data.frame(read.table("uscrime.txt", header = TRUE)) #read in data
> mydata = input[c(16, 1:15)] #reorder so that crime is the first column (for formula)
> f1 = formula(mydata)
> predictors = mydata[-1]
> crime = mydata[1]
> 
> data_train = mydata[1:35,]
> data_test = mydata[36:nrow(mydata),]
> 
> tree = rpart(Crime ~ ., data = data_train)
> 
> summary(tree) # detailed summary of splits
Call:
rpart(formula = Crime ~ ., data = data_train)
  n= 35 

     CP nsplit rel error xerror  xstd
1 0.423      0     1.000  1.060 0.277
2 0.085      1     0.577  0.856 0.175
3 0.010      2     0.492  0.822 0.179

Variable importance
   Po1    Po2 Wealth    Pop     Ed   Ineq   Prob      M 
    28     25     19     10      6      6      3      2 

Node number 1: 35 observations,    complexity param=0.423
  mean=937, MSE=1.72e+05 
  left son=2 (24 obs) right son=3 (11 obs)
  Primary splits:
      Po1  < 10     to the left,  improve=0.423, (0 missing)
      Po2  < 9.1    to the left,  improve=0.402, (0 missing)
      Prob < 0.0418 to the right, improve=0.275, (0 missing)
      NW   < 7.65   to the left,  improve=0.268, (0 missing)
      LF   < 0.541  to the left,  improve=0.214, (0 missing)
  Surrogate splits:
      Po2    < 9.1    to the left,  agree=0.971, adj=0.909, (0 split)
      Wealth < 6230   to the left,  agree=0.914, adj=0.727, (0 split)
      Pop    < 48.5   to the left,  agree=0.800, adj=0.364, (0 split)
      Ed     < 12     to the left,  agree=0.771, adj=0.273, (0 split)
      Ineq   < 15.6   to the right, agree=0.771, adj=0.273, (0 split)

Node number 2: 24 observations,    complexity param=0.085
  mean=755, MSE=5.56e+04 
  left son=4 (15 obs) right son=5 (9 obs)
  Primary splits:
      Po1  < 7.45   to the left,  improve=0.383, (0 missing)
      Po2  < 7.2    to the left,  improve=0.360, (0 missing)
      NW   < 3.1    to the left,  improve=0.268, (0 missing)
      LF   < 0.541  to the left,  improve=0.177, (0 missing)
      Ineq < 21     to the left,  improve=0.144, (0 missing)
  Surrogate splits:
      Po2    < 6.95   to the left,  agree=0.958, adj=0.889, (0 split)
      Wealth < 5330   to the left,  agree=0.833, adj=0.556, (0 split)
      Prob   < 0.0436 to the right, agree=0.833, adj=0.556, (0 split)
      M      < 13.4   to the right, agree=0.792, adj=0.444, (0 split)
      Pop    < 41     to the left,  agree=0.750, adj=0.333, (0 split)

Node number 3: 11 observations
  mean=1.34e+03, MSE=1.94e+05 

Node number 4: 15 observations
  mean=642, MSE=3.63e+04 

Node number 5: 9 observations
  mean=943, MSE=3.1e+04 

> print(tree)
n= 35 

node), split, n, deviance, yval
      * denotes terminal node

1) root 35 6000000  937  
  2) Po1< 10 24 1330000  755  
    4) Po1< 7.45 15  544000  642 *
    5) Po1>=7.45 9  279000  943 *
  3) Po1>=10 11 2130000 1340 *
> 
> plot(tree, uniform = TRUE, main = "Crime Rate Data Decision Tree")
> text(tree,
+      use.n = TRUE,
+      all = TRUE,
+      cex = .8)
> tree$where
 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 
 3  5  3  5  5  5  4  5  3  3  5  4  3  3  3  4  3  5  5  5  3  3  4  4  3  5  3  4  5  3  3  4  3  4  4 
> names = row.names(tree$frame)
> loc = names[tree$where]
> groups = cbind(loc, data_train)
> 
> pred = predict(tree, data_test)
> sse = sum((pred - data_test$Crime) ^ 2)
> sst = sum((data_test$Crime - mean(data_test$Crime)) ^ 2) #total sum of squares
> 1 - sse / sst
[1] -0.28
> # Sum of squares error is greater than the total sum of squares.  This is a bad model.
> 
> # Try pruning to the lowest cross-validation error from the original tree.
> 
> ptree = prune(tree, cp = 0.1481)
> 
> plot(ptree, uniform = TRUE, main = "Crime Rate Data Decision Tree (Pruned)")
> text(ptree,
+      use.n = TRUE,
+      all = TRUE,
+      cex = .8)
> summary(ptree)
Call:
rpart(formula = Crime ~ ., data = data_train)
  n= 35 

     CP nsplit rel error xerror  xstd
1 0.423      0     1.000  1.060 0.277
2 0.148      1     0.577  0.856 0.175

Variable importance
   Po1    Po2 Wealth    Pop     Ed   Ineq 
    28     26     21     10      8      8 

Node number 1: 35 observations,    complexity param=0.423
  mean=937, MSE=1.72e+05 
  left son=2 (24 obs) right son=3 (11 obs)
  Primary splits:
      Po1  < 10     to the left,  improve=0.423, (0 missing)
      Po2  < 9.1    to the left,  improve=0.402, (0 missing)
      Prob < 0.0418 to the right, improve=0.275, (0 missing)
      NW   < 7.65   to the left,  improve=0.268, (0 missing)
      LF   < 0.541  to the left,  improve=0.214, (0 missing)
  Surrogate splits:
      Po2    < 9.1    to the left,  agree=0.971, adj=0.909, (0 split)
      Wealth < 6230   to the left,  agree=0.914, adj=0.727, (0 split)
      Pop    < 48.5   to the left,  agree=0.800, adj=0.364, (0 split)
      Ed     < 12     to the left,  agree=0.771, adj=0.273, (0 split)
      Ineq   < 15.6   to the right, agree=0.771, adj=0.273, (0 split)

Node number 2: 24 observations
  mean=755, MSE=5.56e+04 

Node number 3: 11 observations
  mean=1.34e+03, MSE=1.94e+05 

> print(ptree)
n= 35 

node), split, n, deviance, yval
      * denotes terminal node

1) root 35 6000000  937  
  2) Po1< 10 24 1330000  755 *
  3) Po1>=10 11 2130000 1340 *
> 
> 
> pnames = row.names(ptree$frame)
> ploc = pnames[ptree$where]
> pgroups = cbind(ploc, data_train)
> 
> ppred = predict(ptree, data_test)
> psse = sum((ppred - data_test$Crime) ^ 2) #sum of squared errors
> psst = sum((data_test$Crime - mean(data_test$Crime)) ^ 2) #total sum of squares
> 1 - psse / psst
[1] -0.542
> 
> 
> pgroup2 = subset.data.frame(pgroups, ploc == 2)
> pmodel2 = lm(f1, pgroup2)
> summary(pmodel2)

Call:
lm(formula = f1, data = pgroup2)

Residuals:
   Min     1Q Median     3Q    Max 
-112.0  -57.5    2.1   25.6  199.1 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)  
(Intercept)  -846.542   2890.115   -0.29    0.777  
M              76.386     69.864    1.09    0.306  
So            139.221    143.966    0.97    0.362  
Ed             67.952     89.098    0.76    0.468  
Po1            73.402    186.391    0.39    0.704  
Po2             0.381    202.384    0.00    0.999  
LF            671.615   2363.259    0.28    0.783  
M.F           -32.310     29.404   -1.10    0.304  
Pop             0.974      2.754    0.35    0.733  
NW              0.351      6.216    0.06    0.956  
U1           5357.568   5335.344    1.00    0.345  
U2            -99.206    165.422   -0.60    0.565  
Wealth          0.215      0.170    1.26    0.242  
Ineq           79.134     28.556    2.77    0.024 *
Prob        -6059.122   5492.201   -1.10    0.302  
Time          -19.725     10.344   -1.91    0.093 .
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 134 on 8 degrees of freedom
Multiple R-squared:  0.893,	Adjusted R-squared:  0.691 
F-statistic: 4.43 on 15 and 8 DF,  p-value: 0.0199

> #From the summary, reduce the number of factors by pvalue:
> pmodel2_a = lm(Crime ~ Wealth + Time + M + So + M.F + Prob, pgroup2)
> summary(pmodel2_a) # the model is now worse. I think it is just due to extreme overfitting

Call:
lm(formula = Crime ~ Wealth + Time + M + So + M.F + Prob, data = pgroup2)

Residuals:
   Min     1Q Median     3Q    Max 
-517.2  -80.5   27.4   72.6  325.1 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)  
(Intercept)  2.88e+03   2.78e+03    1.04    0.314  
Wealth       5.75e-02   1.04e-01    0.55    0.589  
Time        -1.94e+01   9.91e+00   -1.96    0.066 .
M            9.84e+01   5.99e+01    1.64    0.119  
So           1.96e+02   1.39e+02    1.41    0.176  
M.F         -2.84e+01   2.37e+01   -1.20    0.247  
Prob        -1.06e+04   4.47e+03   -2.36    0.031 *
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 203 on 17 degrees of freedom
Multiple R-squared:  0.477,	Adjusted R-squared:  0.292 
F-statistic: 2.58 on 6 and 17 DF,  p-value: 0.0579

> 
> 
> pgroup3 = subset.data.frame(pgroups, ploc == 3)
> pmodel3 = lm(f1, pgroup3)
> summary(pmodel3) #This model is so overfit that 5 coeficients are not defined. Reducing the factors gives:

Call:
lm(formula = f1, data = pgroup3)

Residuals:
ALL 11 residuals are 0: no residual degrees of freedom!

Coefficients: (5 not defined because of singularities)
             Estimate Std. Error t value Pr(>|t|)
(Intercept) -7949.186         NA      NA       NA
M             -79.916         NA      NA       NA
So           -948.014         NA      NA       NA
Ed            604.569         NA      NA       NA
Po1          -157.508         NA      NA       NA
Po2           163.191         NA      NA       NA
LF           9965.428         NA      NA       NA
M.F           -30.305         NA      NA       NA
Pop             0.875         NA      NA       NA
NW            150.972         NA      NA       NA
U1          -3404.572         NA      NA       NA
U2                 NA         NA      NA       NA
Wealth             NA         NA      NA       NA
Ineq               NA         NA      NA       NA
Prob               NA         NA      NA       NA
Time               NA         NA      NA       NA

Residual standard error: NaN on 0 degrees of freedom
Multiple R-squared:     1,	Adjusted R-squared:   NaN 
F-statistic:  NaN on 10 and 0 DF,  p-value: NA

> 
> pmodel3_a = lm(Crime ~ M + So + Ed + Po1 + Po2 + LF + M.F + Pop + NW, pgroup2)
> summary(pmodel3_a)

Call:
lm(formula = Crime ~ M + So + Ed + Po1 + Po2 + LF + M.F + Pop + 
    NW, data = pgroup2)

Residuals:
   Min     1Q Median     3Q    Max 
-230.2 -104.5  -24.3  122.9  242.4 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)  
(Intercept) -3800.89    2293.49   -1.66    0.120  
M              81.45      50.46    1.61    0.129  
So            321.16     143.95    2.23    0.043 *
Ed            -83.87      70.64   -1.19    0.255  
Po1           153.77     193.42    0.80    0.440  
Po2            69.20     196.81    0.35    0.730  
LF           3516.90    1601.81    2.20    0.045 *
M.F             7.80      20.46    0.38    0.709  
Pop            -2.68       2.56   -1.05    0.314  
NW             -2.17       6.04   -0.36    0.724  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 171 on 14 degrees of freedom
Multiple R-squared:  0.693,	Adjusted R-squared:  0.495 
F-statistic: 3.51 on 9 and 14 DF,  p-value: 0.0177

> 
> 
> # I would think we are experiencing a large amount
> # of over-fitting since the node, pgroup3, only contains 11 points of the original data, which is very
> # small but still above the 5% minimum (thought 5% of 47 is only 2-3 points of data). The psuedo-R2 calculated
> # from the test data still shows that this model is just not good at all.
