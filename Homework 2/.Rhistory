#Bryson Cook
#ISYE6501, Spring 2018
#Homework 2
#Part 4.1
#install.packages("stats")
#install.packages("ggplot2")
library(stats)
library(ggplot2)
rm(list = ls())
cat("\014")
set.seed(1)
mydata = data.frame(read.csv("iris.csv", header = TRUE)) #read in data
response = data.frame(mydata[, 6])
training = data.matrix(mydata[, 2:5])
#Need to scale the data
sc_train = training
for (i in 1:4) {
sc_train[, i] = (training[, i] - min(training[, i])) / (max(training[, i]) -
min(training[, i]))
}
#Run clustering with all 4 predictors.  "1" cluster isn't really applicable, but it helps with the for loop:
clusters = seq(1, 10)
resp = matrix(, nrow = nrow(sc_train), ncol = length(clusters))
distance = matrix(, nrow = length(clusters), 1)
y = 1
for (x in clusters) {
km = kmeans(sc_train, x, nstart = 20)
resp[, x] = km$cluster
distance[x,] = km$tot.withinss
}
#Comparing the clusters with the known species (though we wouldn't know the actual answer in real life clustering situations)
table(resp[, 2], mydata$Species)
table(resp[, 3], mydata$Species)
table(resp[, 4], mydata$Species)
table(resp[, 5], mydata$Species)
table(resp[, 6], mydata$Species)
table(resp[, 7], mydata$Species)
#Using all four measurements as predictors doesn't really seem to give us much useful information.
#We can keep adding clusters and the clusters will mostly get smaller, but it doesn't actually pertain to much,
#since we have the real data to compare it to.
ggplot(mydata, aes(Petal.Length, Petal.Width, color = resp[, 4])) + geom_point()
ggplot(mydata, aes(Petal.Length, Petal.Width, color = resp[, 4])) + geom_point()
# Plotting out the data vs the known responses shows that the Sepal measurements do not provide a good grouping,
# but the Petal measurements are excellent.  We will redo the previous model creation with only these two predictors.
# We will also only use up to 5 clusters
ggplot(mydata, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point()
ggplot(mydata, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()
ggsave(
"Real Response.jpeg",
width = 6,
height = 4,
units = c("in")
)
#Run clustering with all 4 predictors.  "1" cluster isn't really applicable, but it helps with the for loop:
clusters = seq(1, 5)
resp1 = matrix(, nrow = nrow(sc_train), ncol = length(clusters))
distance1 = matrix(, nrow = length(clusters), 1)
y = 1
for (x in clusters) {
km = kmeans(sc_train[, 3:4], x, nstart = 20)
resp1[, x] = km$cluster
distance1[x,] = km$tot.withinss
}
table(resp1[, 2], mydata$Species)
table(resp1[, 3], mydata$Species)
table(resp1[, 4], mydata$Species)
ggplot(mydata, aes(Petal.Length, Petal.Width, color = resp1[, 2])) + geom_point()
ggsave(
"2 Clusters.jpeg",
width = 6,
height = 4,
units = c("in")
)
ggplot(mydata, aes(Petal.Length, Petal.Width, color = resp1[, 3])) + geom_point()
ggsave(
"3 Clusters.jpeg",
width = 6,
height = 4,
units = c("in")
)
ggplot(mydata, aes(Petal.Length, Petal.Width, color = resp1[, 4])) + geom_point()
ggsave(
"4 Clusters.jpeg",
width = 6,
height = 4,
units = c("in")
)
# As expected, the 3 Cluster solution best matches the actual data, but has 2
# versicolor flowers misclassified and 4 virginica flowers misclassified.
setwd("~/Bryson/Github/ISYE6501/Homework 2")
table(resp1[, 3], mydata$Species)
source('~/Bryson/Github/ISYE6501/Homework 2/HW-4dot1.R', echo=TRUE)
ggplot(mydata, aes(Petal.Length, Petal.Width, color = resp[, 7])) + geom_point()
ggplot(mydata, aes(Petal.Length, Petal.Width, color = resp[, 7])) + geom_point()
table(resp1[, 3], mydata$Species)
#Bryson Cook
#ISYE6501, Spring 2018
#Homework 2
#Part 3.1a
#install.packages("kknn")
library(kknn)
rm(list = ls())
cat("\014")
set.seed(1)
mydata = data.frame(read.csv("credit_card_data.csv", header = FALSE)) #read in data
groups = 10 #10 groups
gs = round(nrow(mydata) / groups, 0) #number of rows per group
steps = seq(1, 20)  #number of neighbors to try
answery = matrix(, length(steps), ncol = groups)
for (y in seq(1, groups)) {
#outer loop to split data into k subsets for crossvalidation
start = 1 + (gs * y) - gs
stop = y * gs
if (y == groups) {
stop = nrow(mydata) #So the last group will include any remainder rows
}
training = mydata[-(start:stop),] #selects all data except the kth group for training
validation = mydata[(start:stop),]  #selects the kth group of data for the validation data
z = 1
for (i in steps) {
#second loop to iterate through neighbor values
answerx = matrix(, nrow(validation), ncol = 1)
for (x in 1:nrow(validation)) {
#inner loop to sweep through data set
knn = kknn(V11 ~ .,
training,
validation[x,],
k = i,
scale = TRUE)
class = round(fitted(knn), 0)
match = class == validation[x, 11]
answerx[x, 1] = match
total = sum(answerx)
}
accuracy = total / nrow(validation)
answery[z, y] = accuracy
z = z + 1
}
}
Best_K_Value = which.max(rowMeans(answery))  #This will chose the training set with the higherst accuracy, thus the best K-NN Value
Best_K_Value
answer_final = matrix(, nrow(mydata), ncol = 1)
for (x in 1:nrow(mydata)) {
#Re-run the chosen model (k-value) on the entire data set to get the accuracy.
knn = kknn(V11 ~ ., mydata[-x,], mydata[x,],
k = Best_K_Value,
scale = TRUE)
class = round(fitted(knn), 0)
match = class == mydata[x, 11]
answer_final[x, 1] = match
}
final_accuracy = sum(answer_final) / nrow(mydata)
final_accuracy
#Bryson Cook
#ISYE6501, Spring 2018
#Homework 2
#Part 3.1b
#install.packages("kknn")
library(kknn)
rm(list = ls())
cat("\014")
set.seed(1)
mydata = data.frame(read.csv("credit_card_data.csv", header = FALSE)) #read in data
training = data.frame()
validation = data.frame()
test = data.frame()
df = data.frame()
count = 1
for (x in 1:nrow(mydata)) {
#using rotation to put the 80% of the data into a training set and 10% each into validation and testing sets
df = mydata[x, ]
if (count >= 11) {
count = 1
}
if (count <= 8) {
training = rbind(training, df)
}  else if (count == 9) {
validation = rbind(validation, df)
}  else if (count == 10) {
test = rbind(test, df)
}
count = count + 1
}
steps = seq(1, 50)
answery = matrix(, nrow = length(steps), ncol = 2)
z = 1
for (y in steps) {
#Using the training data, loop through the validation data to find the best k-nn value
answerx = matrix(, nrow = nrow(validation), ncol = 1)
for (x in 1:nrow(validation)) {
knn = kknn(V11 ~ .,
training,
validation[x, ],
k = y,
scale = TRUE)
class = round(fitted(knn), 0)
match = class == validation[x, 11]
answerx[x, 1] = match
total = sum(answerx)
}
answery[z, 1] = y
accuracy = total / nrow(validation)
answery[z, 2] = accuracy
z = z + 1
}
Best_K_Value_validation = which.max(answery[, 2])  #This will chose the index with higherst accuracy, thus the best K-NN Value
Best_K_Value_validation
answerx = matrix(, nrow = nrow(test), ncol = 1)
for (x in 1:nrow(test)) {
knn = kknn(V11 ~ .,
training,
test[x, ],
k = Best_K_Value_validation,
scale = TRUE)
class = round(fitted(knn), 0)
match = class == test[x, 11]
answerx[x, 1] = match
test_accuracy = sum(answerx) / nrow(test)
}
test_accuracy
#Bryson Cook
#ISYE6501, Spring 2018
#Homework 2
#Part 4.1
#install.packages("stats")
#install.packages("ggplot2")
library(stats)
library(ggplot2)
rm(list = ls())
cat("\014")
set.seed(1)
mydata = data.frame(read.csv("iris.csv", header = TRUE)) #read in data
response = data.frame(mydata[, 6])
training = data.matrix(mydata[, 2:5])
#Need to scale the data
sc_train = training
for (i in 1:4) {
sc_train[, i] = (training[, i] - min(training[, i])) / (max(training[, i]) -
min(training[, i]))
}
#Run clustering with all 4 predictors.  "1" cluster isn't really applicable, but it helps with the for loop:
clusters = seq(1, 10)
resp = matrix(, nrow = nrow(sc_train), ncol = length(clusters))
distance = matrix(, nrow = length(clusters), 1)
y = 1
for (x in clusters) {
km = kmeans(sc_train, x, nstart = 20)
resp[, x] = km$cluster
distance[x,] = km$tot.withinss
}
#Comparing the clusters with the known species (though we wouldn't know the actual answer in real life clustering situations)
table(resp[, 2], mydata$Species)
table(resp[, 3], mydata$Species)
table(resp[, 4], mydata$Species)
table(resp[, 5], mydata$Species)
table(resp[, 6], mydata$Species)
table(resp[, 7], mydata$Species)
#Using all four measurements as predictors doesn't really seem to give us much useful information.
#We can keep adding clusters and the clusters will mostly get smaller, but it doesn't actually pertain to much,
#since we have the real data to compare it to.
ggplot(mydata, aes(Petal.Length, Petal.Width, color = resp[, 7])) + geom_point()
ggplot(mydata, aes(Petal.Length, Petal.Width, color = resp[, 7])) + geom_point()
# Plotting out the data vs the known responses shows that the Sepal measurements do not provide a good grouping,
# but the Petal measurements are excellent.  We will redo the previous model creation with only these two predictors.
# We will also only use up to 5 clusters
ggplot(mydata, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point()
ggplot(mydata, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()
ggsave(
"Real Response.jpeg",
width = 6,
height = 4,
units = c("in")
)
#Run clustering with all 4 predictors.  "1" cluster isn't really applicable, but it helps with the for loop:
clusters = seq(1, 5)
resp1 = matrix(, nrow = nrow(sc_train), ncol = length(clusters))
distance1 = matrix(, nrow = length(clusters), 1)
y = 1
for (x in clusters) {
km = kmeans(sc_train[, 3:4], x, nstart = 20)
resp1[, x] = km$cluster
distance1[x,] = km$tot.withinss
}
ggplot(mydata, aes(Petal.Length, Petal.Width, color = resp1[, 2])) + geom_point()
ggsave(
"2 Clusters.jpeg",
width = 6,
height = 4,
units = c("in")
)
ggplot(mydata, aes(Petal.Length, Petal.Width, color = resp1[, 3])) + geom_point()
ggsave(
"3 Clusters.jpeg",
width = 6,
height = 4,
units = c("in")
)
ggplot(mydata, aes(Petal.Length, Petal.Width, color = resp1[, 4])) + geom_point()
ggsave(
"4 Clusters.jpeg",
width = 6,
height = 4,
units = c("in")
)
table(resp1[, 3], mydata$Species)
# As expected, the 3 Cluster solution best matches the actual data, but has 2
# versicolor flowers misclassified and 4 virginica flowers misclassified.
