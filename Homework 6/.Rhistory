setwd("C:/GitHub/ISYE6501/Homework 6")
# Bryson Cook
# HW 6
# ISYE651, Spring 2018
#Part 9.1
rm(list = ls())
cat("\014")
set.seed(1)
#install.packages("stats")
#install.packages("pls")
#install.packages("DAAG")
library(stats)
library(pls)
library(DAAG)
input = data.frame(read.table("uscrime.txt", header = TRUE)) #read in data
mydata = input[c(16, 1:15)] #reorder so that crime is the first column (for formula)
predictors = mydata[-1]
crime = mydata[1]
PCA = prcomp(~ ., predictors, scale = TRUE)
summary(PCA)
attributes(PCA)
biplot(PCA, scale = 0)
# From the biplot we see that PC1 is a most likely a fuction of Wealth, Income Inequality,
# M, and So and that PC2 is most likely a function of U2, Time, Pop, M.F, and L.F.  These are
# easily becuase these are most parallel to the axes and thus have the largest variances in that direction.
screeplot(PCA, xlab = 'PCs')
var = PCA$sdev ^ 2 #variances
PropofVar = var / sum(var) #Proportion of Variances
plot(PropofVar) #Plots Proportion of Variances vs the PC number.
# From the screeplot and plotting Proportion of Variances vs the PC number, we can visually
# see a diminishing return. I will chose the first 7 PC's, which from the summary we see
# accounts for 92.1% of the values.  Other values can be chosen, but to me this is the point
# after which the curve flattens significantly
n = 7 #this is the number of PC's to use
PCs = PCA$x[, 1:n]
PCdata = cbind(crime, PCs)
model = lm(Crime ~ ., PCdata)
summary(model)
point = data.frame(
M = 14.0,
So = 0,
Ed = 10.0,
Po1 = 12.0,
Po2 = 15.5,
LF = 0.640,
M.F = 94.0,
Pop = 150,
NW = 1.1,
U1 = 0.120,
U2 = 3.6,
Wealth = 3200,
Ineq = 20.1,
Prob = 0.04,
Time = 39.0
)
model$coefficients #These are the coefficients in the PCA (or beta coefficients)
betas = model$coefficients[-1]
beta0 = model$coefficients[1]
#Now we need to rotate/convert the coefficients back into the original unscaled space
alphas = PCA$rotation[, 1:n] %*% betas
p_mean = sapply(predictors, mean)
p_sd = sapply(predictors, sd)
a_orig = alphas / p_sd
a_orig  #these are the un-scaled coefficents for each input
a0 = beta0 - sum(alphas * p_mean / p_sd)
a0 #this is the un-scaled intercept
prediction = a0 + sum(a_orig * point)  #creating the equation for the regression line and calculating the predition
prediction
# The final model from question 8.2 of HW5, which used the formula Crime ~ M + Ed + Po1 + U2 + Ineq + Prob, had
# a predicted crime rate of 1304 and an adjusted R^2 value of 0.766, showing that perhaps there is just too much
# correlation in the data for the PCA model to overcome and that removing multiple predictors as in the last
# homework yields a better model.  However, if we use cross validation:
PClist = as.data.frame(PCA$x[, 1:n])
PCcv = cbind(crime, PClist)
model2 = lm(Crime ~ ., PCcv)
cv = cv.lm(PCcv, model2, m = 5)
mn = mean(crime[, 1])
R2 = 1 - attr(cv, "ms") * nrow(mydata) / sum((crime - mn) ^ 2)
R2
# We calculate an R^2 of 0.456 for the PCA, again using the top 7 PC's, vs 0.413, for the model using
# all 15 predictors.  However, this is when using all 15 variables.  When reducing the number of predictors
# to the above formula, we saw a R^2 of 0.638, which again shows that removing predictors in linear
# regression is superior, at least in this case.
