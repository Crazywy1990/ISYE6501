sc_data = as.matrix(mydata)
for (i in 1:15) { #scale predictors
sc_data[, i] = (mydata[, i] - min(mydata[, i])) / (max(mydata[, i]) -
min(mydata[, i]))
}
predictors = sc_data[,1:15]
response = sc_data[, 16]
resultsmin = matrix(, 19, 2)
colnames(resultsmin) = c('alpha', 'lambda.min')
results1se = matrix(, 19, 2)
colnames(results1se) = c('alpha', 'lambda.1se')
i = 1
for (a in seq(.05,.95,.05)) {
enet = cv.glmnet(
predictors,
response,
family = "gaussian",
alpha = a,
nfolds = 5,
type.measure = "mse"
)
devmin = enet$glmnet.fit$dev.ratio[which(enet$glmnet.fit$lambda == enet$lambda.min)]
dev1se = enet$glmnet.fit$dev.ratio[which(enet$glmnet.fit$lambda == enet$lambda.1se)]
resultsmin[i,1] = a
resultsmin[i,2] = devmin
results1se[i,1] = a
results1se[i,2] = dev1se
i= i+1
}
best_alpha = matrix(, 2, 2)
colnames(best_alpha) = c('alpha', 'lambda')
rownames(best_alpha) = c('.min', '.1se')
best_alpha[1,] = resultsmin[which.max(resultsmin[,2]),]
best_alpha[2,] = results1se[which.max(results1se[,2]),]
best_alpha
r = nrow(sc_data)
set = sample(1:r, size = round(r * .8), replace = FALSE)
train = sc_data[set,]
test = sc_data[-set,]
#Using the lambda.min model
enetmin = cv.glmnet(
predictors,
response,
family = "gaussian",
alpha = best_alpha[1,1],
nfolds = 5,
type.measure = "mse")
coef(enetmin)
model = lm(Crime~M+Po1+Po2+M.F+NW+Ineq+Prob, as.data.frame(train))
summary(model)
pred = predict.lm(model, as.data.frame(test))
sse = sum((pred - test[,16]) ^ 2)
sst = sum((test[,16] - mean(test[,16])) ^ 2) #total sum of squares
1 - sse / sst
#Using the lambda.1se model
enet1se = cv.glmnet(
predictors,
response,
family = "gaussian",
alpha = best_alpha[2,1],
nfolds = 5,
type.measure = "mse")
coef(enet1se)
model1se = lm(Crime~M+Ed+Po1+Po2+M.F+NW+Ineq+Prob, as.data.frame(train))
summary(model1se)
pred1se = predict.lm(model1se, as.data.frame(test))
sse1se = sum((pred1se - test[,16]) ^ 2)
sst1se = sum((test[,16] - mean(test[,16])) ^ 2) #total sum of squares
1 - sse1se / sst1se
cbind(resultsmin, results1se[,2])
cbind(resultsmin, results1se$lambda.1se)
cbind(resultsmin, results1se)
cbind(resultsmin, results1se[,-1])
cbind(resultsmin, results1se)
best_alpha[1,1]
best_alpha[2,1]
# Bryson Cook
# HW 8
# ISYE651, Spring 2018
#Part 11.1.3 - Elastic Net
rm(list = ls())
cat("\014")
set.seed(1)
#install.packages("glmnet")
library(glmnet)
mydata = as.data.frame(read.table("uscrime.txt", header = TRUE)) #read in data
sc_data = as.matrix(mydata)
for (i in 1:15) { #scale predictors
sc_data[, i] = (mydata[, i] - min(mydata[, i])) / (max(mydata[, i]) -
min(mydata[, i]))
}
predictors = sc_data[,1:15]
response = sc_data[, 16]
resultsmin = matrix(, 19, 2)
colnames(resultsmin) = c('alpha', 'lambda.min')
results1se = matrix(, 19, 2)
colnames(results1se) = c('alpha', 'lambda.1se')
i = 1
for (a in seq(.05,.95,.05)) {
enet = cv.glmnet(
predictors,
response,
family = "gaussian",
alpha = a,
nfolds = 5,
type.measure = "mse"
)
devmin = enet$glmnet.fit$dev.ratio[which(enet$glmnet.fit$lambda == enet$lambda.min)]
dev1se = enet$glmnet.fit$dev.ratio[which(enet$glmnet.fit$lambda == enet$lambda.1se)]
resultsmin[i,1] = a
resultsmin[i,2] = devmin
results1se[i,1] = a
results1se[i,2] = dev1se
i= i+1
}
cbind(resultsmin, results1se)
best_alpha = matrix(, 2, 2)
colnames(best_alpha) = c('alpha', 'lambda')
rownames(best_alpha) = c('.min', '.1se')
best_alpha[1,] = resultsmin[which.max(resultsmin[,2]),]
best_alpha[2,] = results1se[which.max(results1se[,2]),]
best_alpha
r = nrow(sc_data)
set = sample(1:r, size = round(r * .8), replace = FALSE)
train = sc_data[set,]
test = sc_data[-set,]
#Using the lambda.min model
enetmin = cv.glmnet(
predictors,
response,
family = "gaussian",
alpha = best_alpha[1,1],
nfolds = 5,
type.measure = "mse")
coef(enetmin)
model = lm(Crime~M+Po1+Po2+M.F+NW+Ineq+Prob, as.data.frame(train))
summary(model)
pred = predict.lm(model, as.data.frame(test))
sse = sum((pred - test[,16]) ^ 2)
sst = sum((test[,16] - mean(test[,16])) ^ 2) #total sum of squares
1 - sse / sst
#Using the lambda.1se model
enet1se = cv.glmnet(
predictors,
response,
family = "gaussian",
alpha = best_alpha[2,1],
nfolds = 5,
type.measure = "mse")
coef(enet1se)
model1se = lm(Crime~M+Ed+Po1+Po2+M.F+NW+Ineq+Prob, as.data.frame(train))
summary(model1se)
pred1se = predict.lm(model1se, as.data.frame(test))
sse1se = sum((pred1se - test[,16]) ^ 2)
sst1se = sum((test[,16] - mean(test[,16])) ^ 2) #total sum of squares
1 - sse1se / sst1se
coef(enetmin, s = lasso$lambda.min)
coef(enetmin, s = enetmin$lambda.min)
coef(enet1se, s = enet1se$lambda.1se)
model = lm(Crime~M+So+Ed+Po1+Po2+LF+M.F+NW+U1+U2+Ineq+Prob, as.data.frame(train))
summary(model)
pred = predict.lm(model, as.data.frame(test))
sse = sum((pred - test[,16]) ^ 2)
sst = sum((test[,16] - mean(test[,16])) ^ 2) #total sum of squares
1 - sse / sst
coef(enet1se, s = enet1se$lambda.1se)
model1se = lm(Crime~M+Ed+Po1+Po2+M.F+NW+Ineq+Prob, as.data.frame(train))
summary(model1se)
pred1se = predict.lm(model1se, as.data.frame(test))
sse1se = sum((pred1se - test[,16]) ^ 2)
sst1se = sum((test[,16] - mean(test[,16])) ^ 2) #total sum of squares
1 - sse1se / sst1se
# Bryson Cook
# HW 8
# ISYE651, Spring 2018
#Part 11.1.3 - Elastic Net
rm(list = ls())
cat("\014")
set.seed(1)
#install.packages("glmnet")
library(glmnet)
mydata = as.data.frame(read.table("uscrime.txt", header = TRUE)) #read in data
sc_data = as.matrix(mydata)
for (i in 1:15) { #scale predictors
sc_data[, i] = (mydata[, i] - min(mydata[, i])) / (max(mydata[, i]) -
min(mydata[, i]))
}
predictors = sc_data[,1:15]
response = sc_data[, 16]
resultsmin = matrix(, 19, 2)
colnames(resultsmin) = c('alpha', 'lambda.min')
results1se = matrix(, 19, 2)
colnames(results1se) = c('alpha', 'lambda.1se')
i = 1
for (a in seq(.05,.95,.05)) {
enet = cv.glmnet(
predictors,
response,
family = "gaussian",
alpha = a,
nfolds = 5,
type.measure = "mse"
)
devmin = enet$glmnet.fit$dev.ratio[which(enet$glmnet.fit$lambda == enet$lambda.min)]
dev1se = enet$glmnet.fit$dev.ratio[which(enet$glmnet.fit$lambda == enet$lambda.1se)]
resultsmin[i,1] = a
resultsmin[i,2] = devmin
results1se[i,1] = a
results1se[i,2] = dev1se
i= i+1
}
cbind(resultsmin, results1se)
best_alpha = matrix(, 2, 2)
colnames(best_alpha) = c('alpha', 'lambda')
rownames(best_alpha) = c('.min', '.1se')
best_alpha[1,] = resultsmin[which.max(resultsmin[,2]),]
best_alpha[2,] = results1se[which.max(results1se[,2]),]
best_alpha
r = nrow(sc_data)
set = sample(1:r, size = round(r * .8), replace = FALSE)
train = sc_data[set,]
test = sc_data[-set,]
#Using the lambda.min model
enetmin = cv.glmnet(
predictors,
response,
family = "gaussian",
alpha = best_alpha[1,1],
nfolds = 5,
type.measure = "mse")
coef(enetmin, s = enetmin$lambda.min)
model = lm(Crime~M+So+Ed+Po1+Po2+LF+M.F+NW+U1+U2+Ineq+Prob, as.data.frame(train))
summary(model)
pred = predict.lm(model, as.data.frame(test))
sse = sum((pred - test[,16]) ^ 2)
sst = sum((test[,16] - mean(test[,16])) ^ 2) #total sum of squares
1 - sse / sst
#Using the lambda.1se model
enet1se = cv.glmnet(
predictors,
response,
family = "gaussian",
alpha = best_alpha[2,1],
nfolds = 5,
type.measure = "mse")
coef(enet1se, s = enet1se$lambda.1se)
model1se = lm(Crime~M+Ed+Po1+Po2+M.F+NW+Ineq+Prob, as.data.frame(train))
summary(model1se)
pred1se = predict.lm(model1se, as.data.frame(test))
sse1se = sum((pred1se - test[,16]) ^ 2)
sst1se = sum((test[,16] - mean(test[,16])) ^ 2) #total sum of squares
1 - sse1se / sst1se
# Bryson Cook
# HW 8
# ISYE651, Spring 2018
#Part 11.1.3 - Elastic Net
rm(list = ls())
cat("\014")
set.seed(1)
#install.packages("glmnet")
library(glmnet)
mydata = as.data.frame(read.table("uscrime.txt", header = TRUE)) #read in data
sc_data = as.matrix(mydata)
for (i in 1:15) { #scale predictors
sc_data[, i] = (mydata[, i] - min(mydata[, i])) / (max(mydata[, i]) -
min(mydata[, i]))
}
predictors = sc_data[,1:15]
response = sc_data[, 16]
resultsmin = matrix(, 19, 2)
colnames(resultsmin) = c('alpha', 'lambda.min')
results1se = matrix(, 19, 2)
colnames(results1se) = c('alpha', 'lambda.1se')
i = 1
for (a in seq(.05,.95,.05)) {
enet = cv.glmnet(
predictors,
response,
family = "gaussian",
alpha = a,
nfolds = 5,
type.measure = "mse"
)
devmin = enet$glmnet.fit$dev.ratio[which(enet$glmnet.fit$lambda == enet$lambda.min)]
dev1se = enet$glmnet.fit$dev.ratio[which(enet$glmnet.fit$lambda == enet$lambda.1se)]
resultsmin[i,1] = a
resultsmin[i,2] = devmin
results1se[i,1] = a
results1se[i,2] = dev1se
i= i+1
}
cbind(resultsmin, results1se)
best_alpha = matrix(, 2, 2)
colnames(best_alpha) = c('alpha', 'lambda')
rownames(best_alpha) = c('.min', '.1se')
best_alpha[1,] = resultsmin[which.max(resultsmin[,2]),]
best_alpha[2,] = results1se[which.max(results1se[,2]),]
best_alpha
r = nrow(sc_data)
set = sample(1:r, size = round(r * .8), replace = FALSE)
train = sc_data[set,]
test = sc_data[-set,]
#Using the lambda.min model
enetmin = cv.glmnet(
predictors,
response,
family = "gaussian",
alpha = best_alpha[1,1],
nfolds = 5,
type.measure = "mse")
coef(enetmin, s = enetmin$lambda.min)
model = lm(Crime~M+So+Ed+Po1+Po2+LF+M.F+NW+U1+U2+Ineq+Prob, as.data.frame(train))
summary(model)
pred = predict.lm(model, as.data.frame(test))
sse = sum((pred - test[,16]) ^ 2)
sst = sum((test[,16] - mean(test[,16])) ^ 2) #total sum of squares
1 - sse / sst
#Using the lambda.1se model
enet1se = cv.glmnet(
predictors,
response,
family = "gaussian",
alpha = best_alpha[2,1],
nfolds = 5,
type.measure = "mse")
coef(enet1se, s = enet1se$lambda.1se)
model1se = lm(Crime~M+Ed+Po1+Po2+M.F+NW+Ineq+Prob, as.data.frame(train))
summary(model1se)
pred1se = predict.lm(model1se, as.data.frame(test))
sse1se = sum((pred1se - test[,16]) ^ 2)
sst1se = sum((test[,16] - mean(test[,16])) ^ 2) #total sum of squares
1 - sse1se / sst1se
# Bryson Cook
# HW 8
# ISYE651, Spring 2018
#Part 11.1.2 - Lasso
rm(list = ls())
cat("\014")
set.seed(1)
#install.packages("glmnet")
library(glmnet)
library(stats)
mydata = as.data.frame(read.table("uscrime.txt", header = TRUE)) #read in data
sc_data = as.matrix(mydata)
for (i in 1:15) {
#scale predictors
sc_data[, i] = (mydata[, i] - min(mydata[, i])) / (max(mydata[, i]) -
min(mydata[, i]))
}
predictors = sc_data[,1:15]
response = sc_data[, 16]
lasso = cv.glmnet(
predictors,
response,
family = "gaussian",
alpha = 1,
nfolds = 5,
type.measure = "mse"
)
lasso$lambda.min
lasso$lambda.1se
plot(lasso)
coef(lasso, s = lasso$lambda.min)
coef(lasso, s = lasso$lambda.1se)
r = nrow(sc_data)
set = sample(1:r, size = round(r * .8), replace = FALSE)
train = sc_data[set,]
test = sc_data[-set,]
# Using the lambda.min model:
model = lm(Crime~M+So+Ed+Po1+U2+Ineq+Prob+Time, as.data.frame(train))
summary(model)
pred = predict.lm(model, as.data.frame(test))
sse = sum((pred - test[,16]) ^ 2)
sst = sum((test[,16] - mean(test[,16])) ^ 2) #total sum of squares
1 - sse / sst
# Using the .1se model, which is the largest value of lambda
# such that error is within 1 standard error of the minimum:
modelse = lm(Crime~M+So+Ed+Po1+M.F+Ineq+Prob, as.data.frame(train))
summary(modelse)
predse = predict.lm(modelse, as.data.frame(test))
ssese = sum((predse - test[,16]) ^ 2)
sstse = sum((test[,16] - mean(test[,16])) ^ 2) #total sum of squares
1 - ssese / sstse
# The .1se model is slightly better, but both should be considered good quality.
# Bryson Cook
# HW 8
# ISYE651, Spring 2018
#Part 11.1.1 - Stepwise Regression
rm(list = ls())
cat("\014")
set.seed(1)
library(stats)
mydata = data.frame(read.table("uscrime.txt", header = TRUE)) #read in data
null = lm(Crime ~ 1, data = mydata)
full = lm(Crime ~ ., data = mydata)
#just using the stepwise selection, let's see what model we get
stepped = step(null,
scope = list(lower = null, upper = full),
direction = "both")
summary(stepped)
# Now we can do it manually, checking the p-values after each step and remove an factors
# that have a p-value >0.05.
sw = step(
null,
scope = list(lower = null, upper = full),
direction = "forward",
steps = 2
)
summary(sw)
sw1 = step(
sw,
scope = list(lower = sw$terms, upper = full),
direction = "forward",
steps = 1
)
summary(sw1)
sw2 = step(
sw1,
scope = list(lower = sw1$terms, upper = full),
direction = "forward",
steps = 1
)
summary(sw2)
sw3 = step(
sw2,
scope = list(lower = sw2$terms, upper = full),
direction = "forward",
steps = 1
)
summary(sw3)
sw4 = step(
sw3,
scope = list(lower = sw3$terms, upper = full),
direction = "forward",
steps = 1
)
summary(sw4)
sw5 = step(
sw4,
scope = list(lower = sw4$terms, upper = full),
direction = "forward",
steps = 1
)
summary(sw5)
# No factors needed to be removed, so it appears using the step() function is the way to go
# in the future.  Double check that the coefficients are the same:
stepped$coefficients
sw5$coefficients
# Bryson Cook
# HW 8
# ISYE651, Spring 2018
#Part 11.1.1 - Stepwise Regression
rm(list = ls())
cat("\014")
set.seed(1)
library(stats)
mydata = data.frame(read.table("uscrime.txt", header = TRUE)) #read in data
null = lm(Crime ~ 1, data = mydata)
full = lm(Crime ~ ., data = mydata)
#just using the stepwise selection, let's see what model we get
stepped = step(null,
scope = list(lower = null, upper = full),
direction = "both")
summary(stepped)
# Now we can do it manually, checking the p-values after each step and remove an factors
# that have a p-value >0.05.
sw = step(
null,
scope = list(lower = null, upper = full),
direction = "forward",
steps = 2
)
summary(sw)
sw1 = step(
sw,
scope = list(lower = sw$terms, upper = full),
direction = "forward",
steps = 1
)
summary(sw1)
sw2 = step(
sw1,
scope = list(lower = sw1$terms, upper = full),
direction = "forward",
steps = 1
)
summary(sw2)
sw3 = step(
sw2,
scope = list(lower = sw2$terms, upper = full),
direction = "forward",
steps = 1
)
summary(sw3)
sw4 = step(
sw3,
scope = list(lower = sw3$terms, upper = full),
direction = "forward",
steps = 1
)
summary(sw4)
sw5 = step(
sw4,
scope = list(lower = sw4$terms, upper = full),
direction = "forward",
steps = 1
)
summary(sw5)
# No factors needed to be removed, so it appears using the step() function is the way to go
# in the future.  Double check that the coefficients are the same:
stepped$coefficients
sw5$coefficients
